{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "Course's pdf\n",
    "<br>\n",
    "https://cs.nyu.edu/courses/spring17/CSCI-UA.0480-009/lecture3-and-half-n-grams.pdf\n",
    "<br>\n",
    "https://www.cs.bgu.ac.il/~elhadad/nlp18/hw1.html#crawl\n",
    "<br>\n",
    "https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf\n",
    "\n",
    "### Code\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/115676_ab6bb49748c742b88127e8b5ce3e1298.html\n",
    "<br>\n",
    "https://appliedmachinelearning.blog/2017/04/30/language-identification-from-texts-using-bi-gram-model-pythonnltk/\n",
    "<br>\n",
    "http://www.albertauyeung.com/post/generating-ngrams-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSc Data Science\n",
    "## Course: Natural Language Proccessing\n",
    "### 1st Assignement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done\n",
    "- Read the data(corpus)\n",
    "- Split the data into sentences (in order to create train, developement, test corpuces)\n",
    "- Split the corpus to training and test data\n",
    "- Add start-end tokens to each sentence of train and development corpuces\n",
    "- Modify the test corpus as a big sentence with start-end tokens\n",
    "- Pre-process the text\n",
    "- Split the corpus into tokens(words)\n",
    "- Create unigram, bigram structures\n",
    "- Create unigram, bigram, trigram frequency structure\n",
    "<br>\n",
    "\n",
    "### Undercunstrction\n",
    "- Decide how we will 'clean' the data (punctuations, capital letters, numbers etc)\n",
    "- Trigram structure\n",
    "<br>\n",
    "\n",
    "### To-do\n",
    "- Deal with OOV(out-of-vocabulary) words\n",
    "- Only words that occur, e.g., at least 10 times in the training subset\n",
    "- Deal with unseen and unknown words\n",
    "- Compute P(sentence) with bigram and trigram language models\n",
    "- Compute entropy and perplexity\n",
    "- Tune the model with the above metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import essential libraries\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#!pip install git+git://github.com/kootenpv/contractions.git\n",
    "import contractions\n",
    "#!pip install inflect\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Europarl corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\nAlthough, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\\nIn the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\nPlease rise, then, for this minute' s silence.\\n(The House rose and observed a minute' s silence)\\nMadam President, on a point of order.\\nYou will be aware from the press and television that there have been a number of bomb explosions and\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the corpus\n",
    "input_path = r'C:\\Users\\User\\Desktop\\MSc Courses\\Untitled Folder\\europarl-v7.el-en.en'\n",
    "circlefile = open (input_path,encoding=\"utf8\")\n",
    "corpus = circlefile.read()\n",
    "circlefile.close()\n",
    "#Print a slice of the corpus\n",
    "corpus[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_corpus = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into train, developement and test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training corpus contains 1045702 sentences\n",
      "The developement corpus contains 130713 sentences\n",
      "The test corpus contains 130713 sentences\n"
     ]
    }
   ],
   "source": [
    "train_corpus_sentences ,test_and_developement_corpus_sentences = train_test_split(sentences_corpus,test_size=0.2)\n",
    "developement_corpus_sentences, test_corpus_sentences = train_test_split(test_and_developement_corpus_sentences,test_size=0.5)\n",
    "\n",
    "print('The training corpus contains {} sentences'.format(len(train_corpus_sentences)))\n",
    "print('The developement corpus contains {} sentences'.format(len(developement_corpus_sentences)))\n",
    "print('The test corpus contains {} sentences'.format(len(test_corpus_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add start-end tokens into the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "START1_TOKEN = '<start1> '\n",
    "START2_TOKEN = '<start2> '\n",
    "END_TOKEN = ' <end1>'\n",
    "\n",
    "def set_start_end_tokens(sentences, start1=START1_TOKEN, start2=START2_TOKEN, end=END_TOKEN):\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentences[i] = start1 + start2 + sent + end\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_sentences = set_start_end_tokens(train_corpus_sentences)\n",
    "developement_corpus_sentences = set_start_end_tokens(developement_corpus_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join the test corpus in one sentence with start-end tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = ' '.join(test_corpus_sentences)\n",
    "test_corpus = START1_TOKEN + START2_TOKEN + test_corpus + END_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "source:\n",
    "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rejoin train and dev corpus in order to pre-process it properly\n",
    "train_corpus = ' '.join(train_corpus_sentences)\n",
    "developement_corpus = ' '.join(developement_corpus_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start1> <start2> If the wind of freedom is blowing through the Arab states then the opportunity must be seized to bring about peace between Israel and Palestine too, so that Israel can live within safe borders but also so that the Palestinian people, who deserve the same dignity as the Israelis, can also live within safe borders. <end> <start1> <start2> Where there are issues of competition involved, it may be a matter for broader consideration but where it is a question of the regulation of excise duties on particular products, this is a matter for the Member States at present and I foresee no change in that. <end> <start1> <start2> The goal of harmonization in education is to enhance European competitiveness and promote the movement of students and researchers. <end> <start1> <start2> Perhaps, as I bring this speech to a close, I might also say something about climate change and compatibility with the environment, which, as I see it, have to do with our society's capacity for innova\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_contractions(text):\n",
    "#     \"\"\"\n",
    "#     Replace contractions in string of text\n",
    "#     e.g. I'm -> I am, Can't -> Can not etc.\n",
    "#     \"\"\"\n",
    "#     return contractions.fix(text)\n",
    "\n",
    "# def remove_between_square_brackets(text):\n",
    "#     return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = remove_between_square_brackets(text)\n",
    "#     text = replace_contractions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~10min\n",
    "#train_corpus = clean_text(train_corpus)\n",
    "#dev_corpus = clean_text(developement_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~ 5min\n",
    "# Tokinize\n",
    "# import re\n",
    "# from nltk.util import ngrams\n",
    "# from nltk import bigrams, trigrams\n",
    "\n",
    "# corpus = corpus.lower()\n",
    "# corpus = re.sub(r'[^a-zA-Z0-9\\s]', ' ', corpus)\n",
    "# tokens = [token for token in corpus.split(\" \") if token != \"\"]\n",
    "# #output = list(ngrams(tokens, 2))\n",
    "train_tokens = nltk.word_tokenize(train_corpus)\n",
    "dev_tokens = nltk.word_tokenize(developement_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'start1', '>', '<', 'start2', '>', 'Then', 'there', 'remained', 'the']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0:10]\n",
    "dev_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    #words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    #words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~2 min\n",
    "train_tokens = normalize(train_tokens)\n",
    "dev_tokens = normalize(dev_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram/Bigram/Trigram Structures\n",
    "\n",
    "usefull sources:\n",
    "https://github.com/ollie283/language-models/blob/master/LangModel.py\n",
    "<br>\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/115676_ab6bb49748c742b88127e8b5ce3e1298.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create n-gram tuples with ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bigrams = nltk.bigrams(tokens)\n",
    "my_trigrams = nltk.trigrams(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequencies of n-gram tuples with ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('council', 'take') 4\n"
     ]
    }
   ],
   "source": [
    "bigram_freq = nltk.FreqDist(my_bigrams)\n",
    "for k,v in bigram_freq.items():\n",
    "    print(k,v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternative custrom structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a list of tokens, return a dictionary where key are the existing unique tokens\n",
    "# and values are their counts; also return the size N of a genre's corpus\n",
    "def get_uniCounts(all_tokens):\n",
    "    unigram_table = {}\n",
    "    for token in all_tokens:\n",
    "        if token in unigram_table:\n",
    "            unigram_table[token] += 1\n",
    "        else:\n",
    "            unigram_table[token] = 1\n",
    "    return unigram_table, len(all_tokens)\n",
    "\n",
    "# given a genre, return a \"dictionary with dictionaries inside\" and the # of seen bigrams\n",
    "# where outside keys are the first words in existing bigrams,\n",
    "# and values are dictionaries with the subsequent word as key\n",
    "# and counts of such bigram as value\n",
    "def get_biCounts(all_tokens):\n",
    "    uniCounts, length = get_uniCounts(all_tokens)\n",
    "    bigram_table = {}\n",
    "    num_bigrams = 0\n",
    "    for x in range(0, length - 1):\n",
    "        if all_tokens[x] in bigram_table:\n",
    "            if all_tokens[x + 1] in bigram_table[all_tokens[x]]:\n",
    "                bigram_table[all_tokens[x]][all_tokens[x + 1]] += 1\n",
    "            else:\n",
    "                bigram_table[all_tokens[x]][all_tokens[x + 1]] = 1\n",
    "                num_bigrams += 1\n",
    "        else:\n",
    "            bigram_table[all_tokens[x]] = {}\n",
    "            bigram_table[all_tokens[x]][all_tokens[x + 1]] = 1\n",
    "            num_bigrams += 1\n",
    "    return bigram_table, num_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~2min\n",
    "table_uniCounts = get_uniCounts(dev_tokens)\n",
    "table_biCounts = get_biCounts(dev_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
