{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "Course's pdf\n",
    "<br>\n",
    "https://cs.nyu.edu/courses/spring17/CSCI-UA.0480-009/lecture3-and-half-n-grams.pdf\n",
    "<br>\n",
    "https://www.cs.bgu.ac.il/~elhadad/nlp18/hw1.html#crawl\n",
    "<br>\n",
    "https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf\n",
    "\n",
    "### Code\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/115676_ab6bb49748c742b88127e8b5ce3e1298.html\n",
    "<br>\n",
    "https://appliedmachinelearning.blog/2017/04/30/language-identification-from-texts-using-bi-gram-model-pythonnltk/\n",
    "<br>\n",
    "http://www.albertauyeung.com/post/generating-ngrams-python/\n",
    "<br>\n",
    "text clean : https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "<br>\n",
    "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSc Data Science\n",
    "## Course: Natural Language Proccessing\n",
    "### 1st Assignement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done\n",
    "- Read the data(corpus)\n",
    "- Split the data into sentences (in order to create train, developement, test corpuces)\n",
    "- Split the corpus to training and test data\n",
    "- Add start-end tokens to each sentence of train and development corpuces\n",
    "- Modify the test corpus as a big sentence with start-end tokens\n",
    "- Pre-process the text\n",
    "- Split the corpus into tokens(words)\n",
    "- Create unigram, bigram structures\n",
    "- Create unigram, bigram, trigram frequency structure\n",
    "<br>\n",
    "\n",
    "### Undercunstrction\n",
    "- Decide how we will 'clean' the data (punctuations, capital letters, numbers etc)\n",
    "- Trigram structure\n",
    "<br>\n",
    "\n",
    "### To-do\n",
    "- Deal with OOV(out-of-vocabulary) words\n",
    "- Only words that occur, e.g., at least 10 times in the training subset\n",
    "- Deal with unseen and unknown words\n",
    "- Compute P(sentence) with bigram and trigram language models\n",
    "- Compute entropy and perplexity\n",
    "- Tune the model with the above metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import essential libraries\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re, string, unicodedata\n",
    "#!pip install git+git://github.com/kootenpv/contractions.git\n",
    "import contractions\n",
    "#!pip install inflect\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Europarl corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\nAlthough, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\\nIn the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\nPlease rise, then, for this minute' s silence.\\n(The House rose and observed a minute' s silence)\\nMadam President, on a point of order.\\nYou will be aware from the press and television that there have been a number of bomb explosions and\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the corpus\n",
    "input_path = r'C:\\Users\\User\\Desktop\\MSc Courses\\Untitled Folder\\europarl-v7.el-en.en'\n",
    "circlefile = open (input_path,encoding=\"utf8\")\n",
    "corpus = circlefile.read()\n",
    "circlefile.close()\n",
    "#Print a slice of the corpus\n",
    "corpus[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~1min\n",
    "sentences_corpus = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split into train, developement and test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training corpus contains 1045702 sentences\n",
      "The developement corpus contains 130713 sentences\n",
      "The test corpus contains 130713 sentences\n"
     ]
    }
   ],
   "source": [
    "train_corpus_sentences ,test_and_developement_corpus_sentences = train_test_split(sentences_corpus,test_size=0.2, random_state = 1)\n",
    "developement_corpus_sentences, test_corpus_sentences = train_test_split(test_and_developement_corpus_sentences,test_size=0.5, random_state = 1)\n",
    "\n",
    "print('The training corpus contains {} sentences'.format(len(train_corpus_sentences)))\n",
    "print('The developement corpus contains {} sentences'.format(len(developement_corpus_sentences)))\n",
    "print('The test corpus contains {} sentences'.format(len(test_corpus_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add start-end tokens into the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "START1_TOKEN = 'start1 '\n",
    "START2_TOKEN = 'start2 '\n",
    "END_TOKEN = ' end1'\n",
    "\n",
    "def set_startNend_tokens_for_trigrams(sentences, start1=START1_TOKEN, start2=START2_TOKEN, end=END_TOKEN):\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentences[i] = start1 + start2 + sent + end\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "def set_startNend_tokens_for_bigrams(sentences, start1=START1_TOKEN, end=END_TOKEN):\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentences[i] = start1 + sent + end\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_sentences = set_startNend_tokens_for_bigrams(train_corpus_sentences)\n",
    "developement_corpus_sentences = set_startNend_tokens_for_bigrams(developement_corpus_sentences)\n",
    "test_corpus_sentences = set_startNend_tokens_for_bigrams(test_corpus_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start1 For this reason my group has tabled an amendment that clearly underlines our common interest in nuclear safety while emphasising that whether Member States invest in nuclear energy must remain their own sovereign decision. end1']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus_sentences[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join the test corpus in one sentence with start-end tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = ' '.join(test_corpus_sentences)\n",
    "test_corpus = START1_TOKEN + test_corpus + END_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "source:\n",
    "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\n",
    "https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rejoin train and dev corpus in order to pre-process it properly\n",
    "train_corpus = ' '.join(train_corpus_sentences)\n",
    "developement_corpus = ' '.join(developement_corpus_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"start1 We must not surrender to the alcohol manufacturers' lobby and its influence, and we must not be afraid of them. end1 start1 (DE) Mr President, ladies and gentlemen, for the Group of the European People's Party (Christian Democrats), the war against terror is the major challenge of the 21st century. end1 start1 Like the rapporteur, I believe that the Green Paper lacks an examination of the socio-economic dimension and an analysis of the expected consequences for the populations concerned, as well as issues of interdependence between individual sectors on impact adaptation. end1 start1 But it also gained massive support from the Catholic hierarchy and clergy, by admission of Cardinal de Courtray himself, on 5 January 1990. end1 start1 We have to recognize that the Commission has moved a long way by bringing forward this inquiry into the dumping of Norwegian salmon. end1 start1 Consumers benefit from being able to compare them. end1 start1 A reduction in the administrative burden u\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~ 5min\n",
    "train_tokens = nltk.word_tokenize(train_corpus)\n",
    "dev_tokens = nltk.word_tokenize(developement_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start1',\n",
       " 'I',\n",
       " 'think',\n",
       " 'this',\n",
       " 'is',\n",
       " 'paramount',\n",
       " '.',\n",
       " 'end1',\n",
       " 'start1',\n",
       " 'Where']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0:10]\n",
    "dev_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):    \n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)   \n",
    "    #words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~2 min\n",
    "train_tokens = normalize(train_tokens)\n",
    "dev_tokens = normalize(dev_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram/Bigram/Trigram Structures\n",
    "\n",
    "usefull sources:\n",
    "https://github.com/ollie283/language-models/blob/master/LangModel.py\n",
    "<br>\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/115676_ab6bb49748c742b88127e8b5ce3e1298.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create uni-gram dict with ntlk and remove tokens with count < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = nltk.FreqDist(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(unigram_freq, train_token):\n",
    "    print('Train Token size: ', len(train_tokens))\n",
    "    replaced_tokens = train_token\n",
    "    for i in range(len(train_tokens)):\n",
    "        if unigram_freq[train_tokens[i]]<10:\n",
    "            replaced_tokens[i] = 'UNK'\n",
    "\n",
    "    replaced_freq = dict(nltk.FreqDist(replaced_tokens))\n",
    "\n",
    "    return replaced_tokens, replaced_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Token size:  27636835\n"
     ]
    }
   ],
   "source": [
    "replaced_tokens, replaced_freq = replace(unigram_freq, train_tokens)\n",
    "#replaced_tokens, replaced_freq = replace(unigram_freq, train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Unigrams, Bigrams, Trigrams Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.util import ngrams\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "my_bigrams = nltk.bigrams(replaced_tokens)\n",
    "my_trigrams = nltk.trigrams(replaced_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~ 1min\n",
    "bigram_freq = nltk.FreqDist(my_bigrams)\n",
    "trigram_freq = nltk.FreqDist(my_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequencies of n-gram tuples with ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_bigrams = {}\n",
    "dict_trigrams = {}\n",
    "for k,v in bigram_freq.items():\n",
    "    bigram_freq[k] = v   \n",
    "    \n",
    "for k,v in trigram_freq.items():\n",
    "    dict_trigrams[k] = v   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'We Do not, like somEthing.!'\n",
    "test_sentence = START2_TOKEN + test_sentence + END_TOKEN\n",
    "test_sentence_tokens = nltk.word_tokenize(test_sentence)\n",
    "test_sentence_tokens = normalize(test_sentence_tokens)\n",
    "test_sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('start1', 'we', 'must'): 8023,\n",
       " ('we', 'must', 'not'): 2048,\n",
       " ('must', 'not', 'surrender'): 3,\n",
       " ('not', 'surrender', 'to'): 3,\n",
       " ('surrender', 'to', 'the'): 7,\n",
       " ('to', 'the', 'alcohol'): 2,\n",
       " ('the', 'alcohol', 'manufacturers'): 1,\n",
       " ('alcohol', 'manufacturers', 'lobby'): 1,\n",
       " ('manufacturers', 'lobby', 'and'): 1,\n",
       " ('lobby', 'and', 'its'): 1,\n",
       " ('and', 'its', 'influence'): 12,\n",
       " ('its', 'influence', 'and'): 7,\n",
       " ('influence', 'and', 'we'): 5,\n",
       " ('and', 'we', 'must'): 1455,\n",
       " ('must', 'not', 'be'): 1691,\n",
       " ('not', 'be', 'afraid'): 72,\n",
       " ('be', 'afraid', 'of'): 38,\n",
       " ('afraid', 'of', 'them'): 2,\n",
       " ('of', 'them', 'end1'): 613,\n",
       " ('them', 'end1', 'start1'): 5610,\n",
       " ('end1', 'start1', 'de'): 4567,\n",
       " ('start1', 'de', 'mr'): 2600,\n",
       " ('de', 'mr', 'president'): 2757,\n",
       " ('mr', 'president', 'ladies'): 5235,\n",
       " ('president', 'ladies', 'and'): 6919,\n",
       " ('ladies', 'and', 'gentlemen'): 14027,\n",
       " ('and', 'gentlemen', 'for'): 90,\n",
       " ('gentlemen', 'for', 'the'): 26,\n",
       " ('for', 'the', 'group'): 115,\n",
       " ('the', 'group', 'of'): 3575,\n",
       " ('group', 'of', 'the'): 3706,\n",
       " ('of', 'the', 'european'): 27013,\n",
       " ('the', 'european', 'people'): 1745,\n",
       " ('european', 'people', 's'): 1619,\n",
       " ('people', 's', 'party'): 1721,\n",
       " ('s', 'party', 'christian'): 955,\n",
       " ('party', 'christian', 'democrats'): 961,\n",
       " ('christian', 'democrats', 'the'): 22,\n",
       " ('democrats', 'the', 'war'): 1,\n",
       " ('the', 'war', 'against'): 59,\n",
       " ('war', 'against', 'terror'): 5,\n",
       " ('against', 'terror', 'is'): 2,\n",
       " ('terror', 'is', 'the'): 1,\n",
       " ('is', 'the', 'major'): 48,\n",
       " ('the', 'major', 'challenge'): 29,\n",
       " ('major', 'challenge', 'of'): 12,\n",
       " ('challenge', 'of', 'the'): 59,\n",
       " ('of', 'the', '21st'): 178,\n",
       " ('the', '21st', 'century'): 507,\n",
       " ('21st', 'century', 'end1'): 217,\n",
       " ('century', 'end1', 'start1'): 539,\n",
       " ('end1', 'start1', 'like'): 702,\n",
       " ('start1', 'like', 'the'): 187,\n",
       " ('like', 'the', 'rapporteur'): 119,\n",
       " ('the', 'rapporteur', 'i'): 141,\n",
       " ('rapporteur', 'i', 'believe'): 20,\n",
       " ('i', 'believe', 'that'): 9451,\n",
       " ('believe', 'that', 'the'): 4023,\n",
       " ('that', 'the', 'green'): 37,\n",
       " ('the', 'green', 'paper'): 729,\n",
       " ('green', 'paper', 'lacks'): 2,\n",
       " ('paper', 'lacks', 'an'): 1,\n",
       " ('lacks', 'an', 'examination'): 1,\n",
       " ('an', 'examination', 'of'): 50,\n",
       " ('examination', 'of', 'the'): 197,\n",
       " ('of', 'the', 'socioeconomic'): 25,\n",
       " ('the', 'socioeconomic', 'dimension'): 1,\n",
       " ('socioeconomic', 'dimension', 'and'): 1,\n",
       " ('dimension', 'and', 'an'): 1,\n",
       " ('and', 'an', 'analysis'): 5,\n",
       " ('an', 'analysis', 'of'): 181,\n",
       " ('analysis', 'of', 'the'): 576,\n",
       " ('of', 'the', 'expected'): 18,\n",
       " ('the', 'expected', 'consequences'): 3,\n",
       " ('expected', 'consequences', 'for'): 1,\n",
       " ('consequences', 'for', 'the'): 315,\n",
       " ('for', 'the', 'populations'): 18,\n",
       " ('the', 'populations', 'concerned'): 12,\n",
       " ('populations', 'concerned', 'as'): 2,\n",
       " ('concerned', 'as', 'well'): 9,\n",
       " ('as', 'well', 'as'): 8251,\n",
       " ('well', 'as', 'issues'): 7,\n",
       " ('as', 'issues', 'of'): 6,\n",
       " ('issues', 'of', 'interdependence'): 1,\n",
       " ('of', 'interdependence', 'between'): 6,\n",
       " ('interdependence', 'between', 'individual'): 1,\n",
       " ('between', 'individual', 'sectors'): 1,\n",
       " ('individual', 'sectors', 'on'): 1,\n",
       " ('sectors', 'on', 'impact'): 1,\n",
       " ('on', 'impact', 'adaptation'): 1,\n",
       " ('impact', 'adaptation', 'end1'): 1,\n",
       " ('adaptation', 'end1', 'start1'): 40,\n",
       " ('end1', 'start1', 'but'): 11276,\n",
       " ('start1', 'but', 'it'): 834,\n",
       " ('but', 'it', 'also'): 249,\n",
       " ('it', 'also', 'gained'): 1,\n",
       " ('also', 'gained', 'massive'): 1,\n",
       " ('gained', 'massive', 'support'): 1,\n",
       " ('massive', 'support', 'from'): 3,\n",
       " ('support', 'from', 'the'): 358,\n",
       " ('from', 'the', 'catholic'): 3,\n",
       " ('the', 'catholic', 'hierarchy'): 1,\n",
       " ('catholic', 'hierarchy', 'and'): 1,\n",
       " ('hierarchy', 'and', 'clergy'): 1,\n",
       " ('and', 'clergy', 'by'): 1,\n",
       " ('clergy', 'by', 'admission'): 1,\n",
       " ('by', 'admission', 'of'): 2,\n",
       " ('admission', 'of', 'cardinal'): 1,\n",
       " ('of', 'cardinal', 'de'): 1,\n",
       " ('cardinal', 'de', 'UNK'): 1,\n",
       " ('de', 'UNK', 'himself'): 1,\n",
       " ('UNK', 'himself', 'on'): 1,\n",
       " ('himself', 'on', '5'): 1,\n",
       " ('on', '5', 'january'): 8,\n",
       " ('5', 'january', '1990'): 1,\n",
       " ('january', '1990', 'end1'): 2,\n",
       " ('1990', 'end1', 'start1'): 96,\n",
       " ('end1', 'start1', 'we'): 72627,\n",
       " ('start1', 'we', 'have'): 10108,\n",
       " ('we', 'have', 'to'): 5723,\n",
       " ('have', 'to', 'recognize'): 34,\n",
       " ('to', 'recognize', 'that'): 66,\n",
       " ('recognize', 'that', 'the'): 58,\n",
       " ('that', 'the', 'commission'): 6158,\n",
       " ('the', 'commission', 'has'): 5119,\n",
       " ('commission', 'has', 'moved'): 5,\n",
       " ('has', 'moved', 'a'): 6,\n",
       " ('moved', 'a', 'long'): 3,\n",
       " ('a', 'long', 'way'): 673,\n",
       " ('long', 'way', 'by'): 1,\n",
       " ('way', 'by', 'bringing'): 1,\n",
       " ('by', 'bringing', 'forward'): 9,\n",
       " ('bringing', 'forward', 'this'): 16,\n",
       " ('forward', 'this', 'inquiry'): 1,\n",
       " ('this', 'inquiry', 'into'): 1,\n",
       " ('inquiry', 'into', 'the'): 86,\n",
       " ('into', 'the', 'dumping'): 1,\n",
       " ('the', 'dumping', 'of'): 25,\n",
       " ('dumping', 'of', 'norwegian'): 2,\n",
       " ('of', 'norwegian', 'salmon'): 10,\n",
       " ('norwegian', 'salmon', 'end1'): 5,\n",
       " ('salmon', 'end1', 'start1'): 27,\n",
       " ('end1', 'start1', 'consumers'): 271,\n",
       " ('start1', 'consumers', 'benefit'): 4,\n",
       " ('consumers', 'benefit', 'from'): 12,\n",
       " ('benefit', 'from', 'being'): 7,\n",
       " ('from', 'being', 'able'): 33,\n",
       " ('being', 'able', 'to'): 579,\n",
       " ('able', 'to', 'compare'): 22,\n",
       " ('to', 'compare', 'them'): 7,\n",
       " ('compare', 'them', 'end1'): 5,\n",
       " ('end1', 'start1', 'a'): 8778,\n",
       " ('start1', 'a', 'reduction'): 43,\n",
       " ('a', 'reduction', 'in'): 777,\n",
       " ('reduction', 'in', 'the'): 645,\n",
       " ('in', 'the', 'administrative'): 43,\n",
       " ('the', 'administrative', 'burden'): 197,\n",
       " ('administrative', 'burden', 'under'): 1,\n",
       " ('burden', 'under', 'eu'): 1,\n",
       " ('under', 'eu', 'legislation'): 3,\n",
       " ('eu', 'legislation', 'end1'): 109,\n",
       " ('legislation', 'end1', 'start1'): 2028,\n",
       " ('end1', 'start1', 'other'): 550,\n",
       " ('start1', 'other', 'issues'): 10,\n",
       " ('other', 'issues', 'that'): 39,\n",
       " ('issues', 'that', 'form'): 1,\n",
       " ('that', 'form', 'part'): 19,\n",
       " ('form', 'part', 'of'): 331,\n",
       " ('part', 'of', 'this'): 501,\n",
       " ('of', 'this', 'revision'): 9,\n",
       " ('this', 'revision', 'are'): 3,\n",
       " ('revision', 'are', 'decentralised'): 1,\n",
       " ('are', 'decentralised', 'local'): 1,\n",
       " ('decentralised', 'local', 'authorities'): 2,\n",
       " ('local', 'authorities', 'prevention'): 1,\n",
       " ('authorities', 'prevention', 'of'): 1,\n",
       " ('prevention', 'of', 'mercenary'): 2,\n",
       " ('of', 'mercenary', 'activities'): 2,\n",
       " ('mercenary', 'activities', 'promotion'): 1,\n",
       " ('activities', 'promotion', 'of'): 1,\n",
       " ('promotion', 'of', 'traditional'): 1,\n",
       " ('of', 'traditional', 'knowledge'): 2,\n",
       " ('traditional', 'knowledge', 'prevention'): 1,\n",
       " ('knowledge', 'prevention', 'of'): 1,\n",
       " ('prevention', 'of', 'hivaids'): 1,\n",
       " ('of', 'hivaids', 'malaria'): 1,\n",
       " ('hivaids', 'malaria', 'and'): 11,\n",
       " ('malaria', 'and', 'tuberculosis'): 28,\n",
       " ('and', 'tuberculosis', 'and'): 1,\n",
       " ('tuberculosis', 'and', 'encouragement'): 1,\n",
       " ('and', 'encouragement', 'of'): 18,\n",
       " ('encouragement', 'of', 'student'): 1,\n",
       " ('of', 'student', 'and'): 2,\n",
       " ('student', 'and', 'youth'): 1,\n",
       " ('and', 'youth', 'exchanges'): 3,\n",
       " ('youth', 'exchanges', 'all'): 1,\n",
       " ('exchanges', 'all', 'of'): 1,\n",
       " ('all', 'of', 'which'): 217,\n",
       " ('of', 'which', 'demonstrates'): 1,\n",
       " ('which', 'demonstrates', 'the'): 15,\n",
       " ('demonstrates', 'the', 'signatories'): 1,\n",
       " ('the', 'signatories', 'greater'): 1,\n",
       " ('signatories', 'greater', 'awareness'): 1,\n",
       " ('greater', 'awareness', 'of'): 26,\n",
       " ('awareness', 'of', 'the'): 283,\n",
       " ('of', 'the', 'specific'): 228,\n",
       " ('the', 'specific', 'problems'): 48,\n",
       " ('specific', 'problems', 'and'): 7,\n",
       " ('problems', 'and', 'aspirations'): 2,\n",
       " ('and', 'aspirations', 'of'): 23,\n",
       " ('aspirations', 'of', 'the'): 57,\n",
       " ('of', 'the', 'people'): 1434,\n",
       " ('the', 'people', 'of'): 2292,\n",
       " ('people', 'of', 'acp'): 1,\n",
       " ('of', 'acp', 'countries'): 26,\n",
       " ('acp', 'countries', 'end1'): 233,\n",
       " ('countries', 'end1', 'start1'): 7433,\n",
       " ('end1', 'start1', 'do'): 1275,\n",
       " ('start1', 'do', 'you'): 482,\n",
       " ('do', 'you', 'agree'): 40,\n",
       " ('you', 'agree', 'with'): 50,\n",
       " ('agree', 'with', 'that'): 112,\n",
       " ('with', 'that', 'prediction'): 1,\n",
       " ('that', 'prediction', 'end1'): 1,\n",
       " ('prediction', 'end1', 'start1'): 3,\n",
       " ('end1', 'start1', 'the'): 132044,\n",
       " ('start1', 'the', 'enlargement'): 51,\n",
       " ('the', 'enlargement', 'of'): 336,\n",
       " ('enlargement', 'of', 'the'): 625,\n",
       " ('the', 'european', 'union'): 44086,\n",
       " ('european', 'union', 'to'): 1731,\n",
       " ('union', 'to', 'include'): 46,\n",
       " ('to', 'include', 'all'): 53,\n",
       " ('include', 'all', 'the'): 39,\n",
       " ('all', 'the', 'applicant'): 36,\n",
       " ('the', 'applicant', 'countries'): 440,\n",
       " ('applicant', 'countries', 'is'): 10,\n",
       " ('countries', 'is', 'an'): 16,\n",
       " ('is', 'an', 'ambition'): 3,\n",
       " ('an', 'ambition', 'that'): 4,\n",
       " ('ambition', 'that', 'will'): 1,\n",
       " ('that', 'will', 'be'): 988,\n",
       " ('will', 'be', 'achieved'): 137,\n",
       " ('be', 'achieved', 'over'): 6,\n",
       " ('achieved', 'over', 'a'): 1,\n",
       " ('over', 'a', 'period'): 127,\n",
       " ('a', 'period', 'of'): 618,\n",
       " ('period', 'of', 'years'): 10,\n",
       " ('of', 'years', 'end1'): 110,\n",
       " ('years', 'end1', 'start1'): 3999,\n",
       " ('start1', 'the', 'principle'): 220,\n",
       " ('the', 'principle', 'of'): 3690,\n",
       " ('principle', 'of', 'subsidiarity'): 817,\n",
       " ('of', 'subsidiarity', 'must'): 22,\n",
       " ('subsidiarity', 'must', 'be'): 19,\n",
       " ('must', 'be', 'correctly'): 4,\n",
       " ('be', 'correctly', 'applied'): 4,\n",
       " ('correctly', 'applied', 'end1'): 15,\n",
       " ('applied', 'end1', 'start1'): 453,\n",
       " ('end1', 'start1', 'i'): 115976,\n",
       " ('start1', 'i', 'believe'): 7147,\n",
       " ('i', 'believe', 'instead'): 3,\n",
       " ('believe', 'instead', 'that'): 5,\n",
       " ('instead', 'that', 'the'): 10,\n",
       " ('that', 'the', 'common'): 224,\n",
       " ('the', 'common', 'interest'): 87,\n",
       " ('common', 'interest', 'of'): 28,\n",
       " ('interest', 'of', 'the'): 238,\n",
       " ('european', 'people', 'should'): 2,\n",
       " ('people', 'should', 'prevail'): 2,\n",
       " ('should', 'prevail', 'end1'): 8,\n",
       " ('prevail', 'end1', 'start1'): 94,\n",
       " ('start1', 'the', 'production'): 41,\n",
       " ('the', 'production', 'of'): 738,\n",
       " ('production', 'of', 'the'): 33,\n",
       " ('of', 'the', 'current'): 1184,\n",
       " ('the', 'current', 'marketing'): 3,\n",
       " ('current', 'marketing', 'year'): 2,\n",
       " ('marketing', 'year', 'was'): 1,\n",
       " ('year', 'was', 'recently'): 1,\n",
       " ('was', 'recently', 'revised'): 1,\n",
       " ('recently', 'revised', 'downwards'): 1,\n",
       " ('revised', 'downwards', 'in'): 3,\n",
       " ('downwards', 'in', 'certain'): 1,\n",
       " ('in', 'certain', 'countries'): 115,\n",
       " ('certain', 'countries', 'including'): 8,\n",
       " ('countries', 'including', 'australia'): 1,\n",
       " ('including', 'australia', 'end1'): 1,\n",
       " ('australia', 'end1', 'start1'): 49,\n",
       " ('start1', 'the', 'european'): 7149,\n",
       " ('european', 'union', 'must'): 1336,\n",
       " ('union', 'must', 'therefore'): 46,\n",
       " ('must', 'therefore', 'limit'): 1,\n",
       " ('therefore', 'limit', 'these'): 1,\n",
       " ('limit', 'these', 'risks'): 2,\n",
       " ('these', 'risks', 'and'): 9,\n",
       " ('risks', 'and', 'shortcomings'): 1,\n",
       " ('and', 'shortcomings', 'by'): 1,\n",
       " ('shortcomings', 'by', 'introducing'): 1,\n",
       " ('by', 'introducing', 'the'): 36,\n",
       " ('introducing', 'the', 'possibility'): 6,\n",
       " ('the', 'possibility', 'of'): 2199,\n",
       " ('possibility', 'of', 'choosing'): 3,\n",
       " ('of', 'choosing', 'the'): 13,\n",
       " ('choosing', 'the', 'applicable'): 3,\n",
       " ('the', 'applicable', 'law'): 24,\n",
       " ('applicable', 'law', 'by'): 1,\n",
       " ('law', 'by', 'mutual'): 1,\n",
       " ('by', 'mutual', 'agreement'): 15,\n",
       " ('mutual', 'agreement', 'between'): 4,\n",
       " ('agreement', 'between', 'the'): 1022,\n",
       " ('between', 'the', 'parties'): 124,\n",
       " ('the', 'parties', 'end1'): 65,\n",
       " ('parties', 'end1', 'start1'): 723,\n",
       " ('end1', 'start1', 'it'): 61057,\n",
       " ('start1', 'it', 'is'): 34907,\n",
       " ('it', 'is', 'natural'): 54,\n",
       " ('is', 'natural', 'that'): 29,\n",
       " ('natural', 'that', 'after'): 1,\n",
       " ('that', 'after', 'a'): 37,\n",
       " ('after', 'a', 'few'): 22,\n",
       " ('a', 'few', 'years'): 446,\n",
       " ('few', 'years', 'of'): 20,\n",
       " ('years', 'of', 'applying'): 1,\n",
       " ('of', 'applying', 'the'): 52,\n",
       " ('applying', 'the', 'third'): 1,\n",
       " ('the', 'third', 'set'): 6,\n",
       " ('third', 'set', 'of'): 7,\n",
       " ('set', 'of', 'guidelines'): 14,\n",
       " ('of', 'guidelines', 'some'): 1,\n",
       " ('guidelines', 'some', 'guidelines'): 1,\n",
       " ('some', 'guidelines', 'became'): 1,\n",
       " ('guidelines', 'became', 'obsolete'): 1,\n",
       " ('became', 'obsolete', 'others'): 1,\n",
       " ('obsolete', 'others', 'were'): 1,\n",
       " ('others', 'were', 'applied'): 1,\n",
       " ('were', 'applied', 'poorly'): 1,\n",
       " ('applied', 'poorly', 'and'): 1,\n",
       " ('poorly', 'and', 'now'): 1,\n",
       " ('and', 'now', 'they'): 20,\n",
       " ('now', 'they', 'need'): 3,\n",
       " ('they', 'need', 'revision'): 1,\n",
       " ('need', 'revision', 'reconsideration'): 1,\n",
       " ('revision', 'reconsideration', 'or'): 1,\n",
       " ('reconsideration', 'or', 'UNK'): 1,\n",
       " ('or', 'UNK', 'abolition'): 1,\n",
       " ('UNK', 'abolition', 'end1'): 1,\n",
       " ('abolition', 'end1', 'start1'): 27,\n",
       " ('start1', 'the', 'proposals'): 204,\n",
       " ('the', 'proposals', 'by'): 35,\n",
       " ('proposals', 'by', 'mr'): 7,\n",
       " ('by', 'mr', 'khanbhai'): 4,\n",
       " ('mr', 'khanbhai', 'for'): 2,\n",
       " ('khanbhai', 'for', 'international'): 1,\n",
       " ('for', 'international', 'recognition'): 2,\n",
       " ('international', 'recognition', 'by'): 1,\n",
       " ('recognition', 'by', 'the'): 20,\n",
       " ('by', 'the', 'eu'): 908,\n",
       " ('the', 'eu', 'of'): 105,\n",
       " ('eu', 'of', 'the'): 28,\n",
       " ('of', 'the', 'right'): 424,\n",
       " ('the', 'right', 'of'): 1283,\n",
       " ('right', 'of', 'developing'): 2,\n",
       " ('of', 'developing', 'countries'): 225,\n",
       " ('developing', 'countries', 'to'): 180,\n",
       " ('countries', 'to', 'produce'): 8,\n",
       " ('to', 'produce', 'generic'): 6,\n",
       " ('produce', 'generic', 'treatments'): 1,\n",
       " ('generic', 'treatments', 'for'): 1,\n",
       " ('treatments', 'for', 'epidemic'): 1,\n",
       " ('for', 'epidemic', 'diseases'): 1,\n",
       " ('epidemic', 'diseases', 'without'): 1,\n",
       " ('diseases', 'without', 'having'): 1,\n",
       " ('without', 'having', 'to'): 107,\n",
       " ('having', 'to', 'pay'): 50,\n",
       " ('to', 'pay', 'the'): 166,\n",
       " ('pay', 'the', 'costs'): 13,\n",
       " ('the', 'costs', 'of'): 516,\n",
       " ('costs', 'of', 'intellectual'): 2,\n",
       " ('of', 'intellectual', 'property'): 279,\n",
       " ('intellectual', 'property', 'rights'): 357,\n",
       " ('property', 'rights', 'as'): 7,\n",
       " ('rights', 'as', 'well'): 88,\n",
       " ('well', 'as', 'the'): 2017,\n",
       " ('as', 'the', 'stimulation'): 1,\n",
       " ('the', 'stimulation', 'of'): 14,\n",
       " ('stimulation', 'of', 'production'): 2,\n",
       " ('of', 'production', 'of'): 13,\n",
       " ('production', 'of', 'treatments'): 1,\n",
       " ('of', 'treatments', 'for'): 3,\n",
       " ('treatments', 'for', 'diseases'): 2,\n",
       " ('for', 'diseases', 'caused'): 1,\n",
       " ('diseases', 'caused', 'by'): 18,\n",
       " ('caused', 'by', 'poverty'): 3,\n",
       " ('by', 'poverty', 'and'): 16,\n",
       " ('poverty', 'and', 'the'): 79,\n",
       " ('and', 'the', 'periodical'): 1,\n",
       " ('the', 'periodical', 'review'): 1,\n",
       " ('periodical', 'review', 'of'): 1,\n",
       " ('review', 'of', 'the'): 1017,\n",
       " ('of', 'the', 'trips'): 16,\n",
       " ('the', 'trips', 'agreement'): 61,\n",
       " ('trips', 'agreement', 'are'): 1,\n",
       " ('agreement', 'are', 'essential'): 1,\n",
       " ('are', 'essential', 'to'): 112,\n",
       " ('essential', 'to', 'reach'): 6,\n",
       " ('to', 'reach', 'a'): 598,\n",
       " ('reach', 'a', 'solution'): 42,\n",
       " ('a', 'solution', 'end1'): 241,\n",
       " ('solution', 'end1', 'start1'): 1102,\n",
       " ('end1', 'start1', 'otherwise'): 729,\n",
       " ('start1', 'otherwise', 'the'): 140,\n",
       " ('otherwise', 'the', 'final'): 2,\n",
       " ('the', 'final', 'report'): 104,\n",
       " ('final', 'report', 'of'): 20,\n",
       " ('report', 'of', 'such'): 1,\n",
       " ('of', 'such', 'committees'): 3,\n",
       " ('such', 'committees', 'will'): 1,\n",
       " ('committees', 'will', 'be'): 5,\n",
       " ('will', 'be', 'based'): 111,\n",
       " ('be', 'based', 'on'): 917,\n",
       " ('based', 'on', 'incomplete'): 2,\n",
       " ('on', 'incomplete', 'investigation'): 1,\n",
       " ('incomplete', 'investigation', 'and'): 1,\n",
       " ('investigation', 'and', 'as'): 1,\n",
       " ('and', 'as', 'in'): 21,\n",
       " ('as', 'in', 'this'): 60,\n",
       " ('in', 'this', 'case'): 1803,\n",
       " ('this', 'case', 'the'): 264,\n",
       " ('case', 'the', 'excellent'): 1,\n",
       " ('the', 'excellent', 'work'): 321,\n",
       " ('excellent', 'work', 'undertaken'): 2,\n",
       " ('work', 'undertaken', 'by'): 34,\n",
       " ('undertaken', 'by', 'colleagues'): 1,\n",
       " ('by', 'colleagues', 'sitting'): 1,\n",
       " ('colleagues', 'sitting', 'on'): 1,\n",
       " ('sitting', 'on', 'these'): 4,\n",
       " ('on', 'these', 'committees'): 4,\n",
       " ('these', 'committees', 'will'): 1,\n",
       " ('will', 'be', 'devalued'): 2,\n",
       " ('be', 'devalued', 'by'): 1,\n",
       " ('devalued', 'by', 'factors'): 1,\n",
       " ('by', 'factors', 'beyond'): 1,\n",
       " ('factors', 'beyond', 'their'): 2,\n",
       " ('beyond', 'their', 'control'): 10,\n",
       " ('their', 'control', 'end1'): 14,\n",
       " ('control', 'end1', 'start1'): 944,\n",
       " ('end1', 'start1', 'in'): 54039,\n",
       " ('start1', 'in', 'this'): 5559,\n",
       " ('in', 'this', 'respect'): 2635,\n",
       " ('this', 'respect', 'and'): 112,\n",
       " ('respect', 'and', 'let'): 1,\n",
       " ('and', 'let', 'me'): 88,\n",
       " ('let', 'me', 'point'): 85,\n",
       " ('me', 'point', 'out'): 80,\n",
       " ('point', 'out', 'that'): 2386,\n",
       " ('out', 'that', 'i'): 91,\n",
       " ('that', 'i', 'am'): 1472,\n",
       " ('i', 'am', 'not'): 2373,\n",
       " ('am', 'not', 'known'): 3,\n",
       " ('not', 'known', 'as'): 1,\n",
       " ('known', 'as', 'a'): 33,\n",
       " ('as', 'a', 'leftwing'): 3,\n",
       " ('a', 'leftwing', 'UNK'): 1,\n",
       " ('leftwing', 'UNK', 'i'): 1,\n",
       " ('UNK', 'i', 'have'): 42,\n",
       " ('i', 'have', 'to'): 1979,\n",
       " ('have', 'to', 'say'): 1434,\n",
       " ('to', 'say', 'that'): 4632,\n",
       " ('say', 'that', 'i'): 1246,\n",
       " ('i', 'am', 'surprised'): 137,\n",
       " ('am', 'surprised', 'and'): 5,\n",
       " ('surprised', 'and', 'dismayed'): 1,\n",
       " ('and', 'dismayed', 'by'): 1,\n",
       " ('dismayed', 'by', 'the'): 7,\n",
       " ('by', 'the', 'amendment'): 13,\n",
       " ('the', 'amendment', 'tabled'): 110,\n",
       " ('amendment', 'tabled', 'by'): 152,\n",
       " ('tabled', 'by', 'our'): 61,\n",
       " ('by', 'our', 'fellow'): 78,\n",
       " ('our', 'fellow', 'member'): 150,\n",
       " ('fellow', 'member', 'mr'): 223,\n",
       " ('member', 'mr', 'weber'): 2,\n",
       " ('mr', 'weber', 'whom'): 1,\n",
       " ('weber', 'whom', 'i'): 1,\n",
       " ('whom', 'i', 'respect'): 5,\n",
       " ('i', 'respect', 'and'): 2,\n",
       " ('respect', 'and', 'several'): 1,\n",
       " ('and', 'several', 'others'): 9,\n",
       " ('several', 'others', 'end1'): 3,\n",
       " ('others', 'end1', 'start1'): 1103,\n",
       " ('end1', 'start1', 'however'): 19415,\n",
       " ('start1', 'however', 'this'): 971,\n",
       " ('however', 'this', 'is'): 241,\n",
       " ('this', 'is', 'only'): 178,\n",
       " ('is', 'only', 'one'): 290,\n",
       " ('only', 'one', 'tool'): 2,\n",
       " ('one', 'tool', 'in'): 3,\n",
       " ('tool', 'in', 'the'): 57,\n",
       " ('in', 'the', 'struggle'): 66,\n",
       " ('the', 'struggle', 'to'): 35,\n",
       " ('struggle', 'to', 'stop'): 1,\n",
       " ('to', 'stop', 'illegal'): 4,\n",
       " ('stop', 'illegal', 'immigration'): 1,\n",
       " ('illegal', 'immigration', 'end1'): 251,\n",
       " ('immigration', 'end1', 'start1'): 543,\n",
       " ('end1', 'start1', 'regardless'): 110,\n",
       " ('start1', 'regardless', 'of'): 110,\n",
       " ('regardless', 'of', 'future'): 1,\n",
       " ('of', 'future', 'changes'): 2,\n",
       " ('future', 'changes', 'to'): 6,\n",
       " ('changes', 'to', 'how'): 1,\n",
       " ('to', 'how', 'the'): 101,\n",
       " ('how', 'the', 'union'): 27,\n",
       " ('the', 'union', 'is'): 579,\n",
       " ('union', 'is', 'represented'): 4,\n",
       " ('is', 'represented', 'within'): 3,\n",
       " ('represented', 'within', 'the'): 9,\n",
       " ('within', 'the', 'un'): 37,\n",
       " ('the', 'un', 'and'): 180,\n",
       " ('un', 'and', 'in'): 5,\n",
       " ('and', 'in', 'the'): 3075,\n",
       " ('in', 'the', 'security'): 114,\n",
       " ('the', 'security', 'council'): 315,\n",
       " ('security', 'council', 'a'): 2,\n",
       " ('council', 'a', 'debate'): 1,\n",
       " ('a', 'debate', 'which'): 62,\n",
       " ('debate', 'which', 'is'): 73,\n",
       " ('which', 'is', 'still'): 225,\n",
       " ('is', 'still', 'open'): 23,\n",
       " ('still', 'open', 'in'): 1,\n",
       " ('open', 'in', 'particular'): 1,\n",
       " ('in', 'particular', 'there'): 53,\n",
       " ('particular', 'there', 'is'): 24,\n",
       " ('there', 'is', 'increasingly'): 8,\n",
       " ('is', 'increasingly', 'a'): 9,\n",
       " ('increasingly', 'a', 'need'): 1,\n",
       " ('a', 'need', 'to'): 557,\n",
       " ('need', 'to', 'join'): 15,\n",
       " ('to', 'join', 'up'): 6,\n",
       " ('join', 'up', 'the'): 1,\n",
       " ('up', 'the', 'efforts'): 5,\n",
       " ('the', 'efforts', 'of'): 498,\n",
       " ('efforts', 'of', 'european'): 4,\n",
       " ('of', 'european', 'countries'): 97,\n",
       " ('european', 'countries', 'so'): 7,\n",
       " ('countries', 'so', 'as'): 16,\n",
       " ('so', 'as', 'to'): 1883,\n",
       " ('as', 'to', 'alongside'): 1,\n",
       " ('to', 'alongside', 'the'): 1,\n",
       " ('alongside', 'the', 'union'): 1,\n",
       " ('the', 'union', 's'): 2240,\n",
       " ('union', 's', 'representatives'): 7,\n",
       " ('s', 'representatives', 'overseas'): 1,\n",
       " ('representatives', 'overseas', 'encourage'): 1,\n",
       " ('overseas', 'encourage', 'a'): 1,\n",
       " ('encourage', 'a', 'collective'): 1,\n",
       " ('a', 'collective', 'capacity'): 1,\n",
       " ('collective', 'capacity', 'for'): 1,\n",
       " ('capacity', 'for', 'intervention'): 5,\n",
       " ('for', 'intervention', 'and'): 6,\n",
       " ('intervention', 'and', 'influence'): 1,\n",
       " ('and', 'influence', 'end1'): 8,\n",
       " ('influence', 'end1', 'start1'): 220,\n",
       " ('start1', 'i', 'would'): 16419,\n",
       " ('i', 'would', 'like'): 21793,\n",
       " ('would', 'like', 'to'): 22809,\n",
       " ('like', 'to', 'thank'): 5991,\n",
       " ('to', 'thank', 'you'): 809,\n",
       " ('thank', 'you', 'for'): 2042,\n",
       " ('you', 'for', 'your'): 974,\n",
       " ('for', 'your', 'support'): 178,\n",
       " ('your', 'support', 'and'): 50,\n",
       " ('support', 'and', 'i'): 77,\n",
       " ('and', 'i', 'would'): 2587,\n",
       " ('i', 'would', 'also'): 3329,\n",
       " ('would', 'also', 'like'): 2703,\n",
       " ('also', 'like', 'to'): 3546,\n",
       " ('thank', 'you', 'in'): 36,\n",
       " ('you', 'in', 'the'): 154,\n",
       " ('in', 'the', 'name'): 686,\n",
       " ('the', 'name', 'of'): 870,\n",
       " ('name', 'of', 'my'): 14,\n",
       " ('of', 'my', 'colleague'): 181,\n",
       " ('my', 'colleague', 'johannes'): 2,\n",
       " ('colleague', 'johannes', 'hahn'): 2,\n",
       " ('johannes', 'hahn', 'who'): 1,\n",
       " ('hahn', 'who', 'has'): 1,\n",
       " ('who', 'has', 'responsibility'): 10,\n",
       " ('has', 'responsibility', 'for'): 21,\n",
       " ('responsibility', 'for', 'this'): 155,\n",
       " ('for', 'this', 'area'): 49,\n",
       " ('this', 'area', 'end1'): 1717,\n",
       " ('area', 'end1', 'start1'): 3508,\n",
       " ('end1', 'start1', 'if'): 13367,\n",
       " ('start1', 'if', 'we'): 3690,\n",
       " ('if', 'we', 'do'): 1162,\n",
       " ('we', 'do', 'not'): 5967,\n",
       " ('do', 'not', 'have'): 2316,\n",
       " ('not', 'have', 'this'): 60,\n",
       " ('have', 'this', 'effective'): 1,\n",
       " ('this', 'effective', 'control'): 1,\n",
       " ('effective', 'control', 'of'): 34,\n",
       " ('control', 'of', 'illegal'): 5,\n",
       " ('of', 'illegal', 'immigration'): 147,\n",
       " ('illegal', 'immigration', 'because'): 4,\n",
       " ('immigration', 'because', 'there'): 1,\n",
       " ('because', 'there', 'is'): 490,\n",
       " ('there', 'is', 'insufficient'): 43,\n",
       " ('is', 'insufficient', 'cooperation'): 2,\n",
       " ('insufficient', 'cooperation', 'with'): 1,\n",
       " ('cooperation', 'with', 'other'): 62,\n",
       " ('with', 'other', 'countries'): 165,\n",
       " ('other', 'countries', 'because'): 13,\n",
       " ('countries', 'because', 'there'): 3,\n",
       " ('because', 'there', 'are'): 332,\n",
       " ('there', 'are', 'no'): 1087,\n",
       " ('are', 'no', 'readmission'): 1,\n",
       " ('no', 'readmission', 'agreements'): 1,\n",
       " ('readmission', 'agreements', 'those'): 1,\n",
       " ('agreements', 'those', 'borders'): 1,\n",
       " ('those', 'borders', 'are'): 2,\n",
       " ('borders', 'are', 'weakened'): 1,\n",
       " ('are', 'weakened', 'in'): 1,\n",
       " ('weakened', 'in', 'practice'): 1,\n",
       " ('in', 'practice', 'end1'): 403,\n",
       " ('practice', 'end1', 'start1'): 1034,\n",
       " ('end1', 'start1', 'while'): 1774,\n",
       " ('start1', 'while', 'the'): 326,\n",
       " ('while', 'the', 'market'): 3,\n",
       " ('the', 'market', 'for'): 180,\n",
       " ('market', 'for', 'mortgage'): 5,\n",
       " ('for', 'mortgage', 'UNK'): 1,\n",
       " ('mortgage', 'UNK', 'bonds'): 1,\n",
       " ('UNK', 'bonds', 'is'): 1,\n",
       " ('bonds', 'is', 'strong'): 1,\n",
       " ('is', 'strong', 'in'): 6,\n",
       " ('strong', 'in', 'many'): 2,\n",
       " ('in', 'many', 'eu'): 45,\n",
       " ('many', 'eu', 'countries'): 45,\n",
       " ('eu', 'countries', 'there'): 8,\n",
       " ('countries', 'there', 'are'): 51,\n",
       " ('there', 'are', 'some'): 625,\n",
       " ('are', 'some', 'countries'): 13,\n",
       " ('some', 'countries', 'where'): 7,\n",
       " ('countries', 'where', 'they'): 56,\n",
       " ('where', 'they', 'do'): 24,\n",
       " ('they', 'do', 'not'): 1373,\n",
       " ('do', 'not', 'exist'): 103,\n",
       " ('not', 'exist', 'at'): 17,\n",
       " ('exist', 'at', 'all'): 14,\n",
       " ('at', 'all', 'end1'): 881,\n",
       " ('all', 'end1', 'start1'): 2099,\n",
       " ('it', 'is', 'worth'): 525,\n",
       " ('is', 'worth', 'pointing'): 49,\n",
       " ('worth', 'pointing', 'out'): 64,\n",
       " ('pointing', 'out', 'that'): 186,\n",
       " ('out', 'that', 'this'): 203,\n",
       " ('that', 'this', 'is'): 3826,\n",
       " ('this', 'is', 'possible'): 74,\n",
       " ('is', 'possible', 'as'): 13,\n",
       " ('possible', 'as', 'has'): 2,\n",
       " ('as', 'has', 'been'): 825,\n",
       " ('has', 'been', 'demonstrated'): 61,\n",
       " ('been', 'demonstrated', 'by'): 19,\n",
       " ('demonstrated', 'by', 'the'): 147,\n",
       " ('by', 'the', 'progress'): 15,\n",
       " ('the', 'progress', 'made'): 429,\n",
       " ('progress', 'made', 'in'): 209,\n",
       " ('made', 'in', 'free'): 1,\n",
       " ('in', 'free', 'trade'): 10,\n",
       " ('free', 'trade', 'agreements'): 167,\n",
       " ('trade', 'agreements', 'end1'): 111,\n",
       " ('agreements', 'end1', 'start1'): 1140,\n",
       " ('start1', 'if', 'western'): 2,\n",
       " ('if', 'western', 'UNK'): 1,\n",
       " ('western', 'UNK', 'is'): 1,\n",
       " ('UNK', 'is', 'having'): 2,\n",
       " ('is', 'having', 'an'): 24,\n",
       " ('having', 'an', 'identity'): 1,\n",
       " ('an', 'identity', 'crisis'): 11,\n",
       " ('identity', 'crisis', 'it'): 1,\n",
       " ('crisis', 'it', 'is'): 124,\n",
       " ('it', 'is', 'because'): 328,\n",
       " ('is', 'because', 'the'): 185,\n",
       " ('because', 'the', 'fashion'): 1,\n",
       " ('the', 'fashion', 'for'): 4,\n",
       " ('fashion', 'for', 'multiculturalism'): 1,\n",
       " ('for', 'multiculturalism', 'has'): 1,\n",
       " ('multiculturalism', 'has', 'led'): 1,\n",
       " ('has', 'led', 'so'): 2,\n",
       " ('led', 'so', 'many'): 1,\n",
       " ('so', 'many', 'of'): 81,\n",
       " ('many', 'of', 'us'): 458,\n",
       " ('of', 'us', 'to'): 149,\n",
       " ('us', 'to', 'avoid'): 41,\n",
       " ('to', 'avoid', 'tackling'): 3,\n",
       " ('avoid', 'tackling', 'these'): 1,\n",
       " ('tackling', 'these', 'issues'): 6,\n",
       " ('these', 'issues', 'which'): 33,\n",
       " ('issues', 'which', 'are'): 124,\n",
       " ('which', 'are', 'at'): 101,\n",
       " ('are', 'at', 'the'): 372,\n",
       " ('at', 'the', 'heart'): 746,\n",
       " ('the', 'heart', 'of'): 997,\n",
       " ('heart', 'of', 'the'): 527,\n",
       " ('of', 'the', 'battles'): 4,\n",
       " ('the', 'battles', 'faced'): 1,\n",
       " ('battles', 'faced', 'by'): 1,\n",
       " ('faced', 'by', 'so'): 1,\n",
       " ('by', 'so', 'many'): 41,\n",
       " ('so', 'many', 'muslim'): 1,\n",
       " ('many', 'muslim', 'women'): 1,\n",
       " ('muslim', 'women', 'in'): 4,\n",
       " ('women', 'in', 'europe'): 68,\n",
       " ('in', 'europe', 'and'): 1274,\n",
       " ('europe', 'and', 'the'): 835,\n",
       " ('and', 'the', 'world'): 337,\n",
       " ('the', 'world', 'end1'): 2413,\n",
       " ('world', 'end1', 'start1'): 3433,\n",
       " ('end1', 'start1', 'that'): 20732,\n",
       " ('start1', 'that', 'means'): 521,\n",
       " ('that', 'means', 'that'): 375,\n",
       " ('means', 'that', 'we'): 646,\n",
       " ('that', 'we', 'are'): 6157,\n",
       " ('we', 'are', 'supporting'): 64,\n",
       " ('are', 'supporting', 'the'): 42,\n",
       " ('supporting', 'the', 'struggle'): 4,\n",
       " ('the', 'struggle', 'of'): 19,\n",
       " ('struggle', 'of', 'the'): 22,\n",
       " ('of', 'the', 'moderate'): 9,\n",
       " ('the', 'moderate', 'majority'): 3,\n",
       " ('moderate', 'majority', 'of'): 1,\n",
       " ('majority', 'of', 'pakistanis'): 4,\n",
       " ('of', 'pakistanis', 'against'): 1,\n",
       " ('pakistanis', 'against', 'the'): 1,\n",
       " ('against', 'the', 'violent'): 1,\n",
       " ('the', 'violent', 'minority'): 1,\n",
       " ('violent', 'minority', 'of'): 1,\n",
       " ('minority', 'of', 'extremists'): 1,\n",
       " ('of', 'extremists', 'end1'): 3,\n",
       " ('extremists', 'end1', 'start1'): 51,\n",
       " ('end1', 'start1', 'and'): 5975,\n",
       " ('start1', 'and', 'is'): 14,\n",
       " ('and', 'is', 'odysseus'): 1,\n",
       " ('is', 'odysseus', 'also'): 1,\n",
       " ('odysseus', 'also', 'going'): 1,\n",
       " ('also', 'going', 'to'): 89,\n",
       " ('going', 'to', 'make'): 154,\n",
       " ('to', 'make', 'this'): 493,\n",
       " ('make', 'this', 'clear'): 51,\n",
       " ('this', 'clear', 'to'): 13,\n",
       " ('clear', 'to', 'the'): 174,\n",
       " ('to', 'the', 'civil'): 26,\n",
       " ('the', 'civil', 'servants'): 15,\n",
       " ('civil', 'servants', 'which'): 1,\n",
       " ('servants', 'which', 'member'): 1,\n",
       " ('which', 'member', 'states'): 192,\n",
       " ('member', 'states', 'are'): 2011,\n",
       " ('states', 'are', 'increasingly'): 5,\n",
       " ('are', 'increasingly', 'posting'): 1,\n",
       " ('increasingly', 'posting', 'at'): 1,\n",
       " ('posting', 'at', 'the'): 1,\n",
       " ('at', 'the', 'airports'): 7,\n",
       " ('the', 'airports', 'of'): 10,\n",
       " ('airports', 'of', 'the'): 8,\n",
       " ('of', 'the', 'countries'): 700,\n",
       " ('the', 'countries', 'of'): 1739,\n",
       " ('countries', 'of', 'origin'): 324,\n",
       " ('of', 'origin', 'of'): 85,\n",
       " ('origin', 'of', 'asylum'): 2,\n",
       " ('of', 'asylum', 'seekers'): 163,\n",
       " ('asylum', 'seekers', 'as'): 10,\n",
       " ('seekers', 'as', 'extended'): 1,\n",
       " ('as', 'extended', 'forward'): 1,\n",
       " ('extended', 'forward', 'border'): 1,\n",
       " ('forward', 'border', 'posts'): 1,\n",
       " ('border', 'posts', 'of'): 1,\n",
       " ('posts', 'of', 'fortress'): 1,\n",
       " ('of', 'fortress', 'europe'): 20,\n",
       " ('fortress', 'europe', 'end1'): 30,\n",
       " ('europe', 'end1', 'start1'): 9129,\n",
       " ('start1', 'i', 'feel'): 694,\n",
       " ('i', 'feel', 'that'): 1033,\n",
       " ('feel', 'that', 'because'): 1,\n",
       " ('that', 'because', 'of'): 123,\n",
       " ('because', 'of', 'completely'): 1,\n",
       " ('of', 'completely', 'misinterpreted'): 1,\n",
       " ('completely', 'misinterpreted', 'or'): 1,\n",
       " ('misinterpreted', 'or', 'poorly'): 1,\n",
       " ('or', 'poorly', 'translated'): 1,\n",
       " ('poorly', 'translated', 'texts'): 1,\n",
       " ('translated', 'texts', 'for'): 1,\n",
       " ('texts', 'for', 'example'): 1,\n",
       " ('for', 'example', 'the'): 1360,\n",
       " ('example', 'the', 'right'): 19,\n",
       " ('the', 'right', 'to'): 3554,\n",
       " ('right', 'to', 'vote'): 149,\n",
       " ('to', 'vote', 'which'): 2,\n",
       " ('vote', 'which', 'is'): 21,\n",
       " ('which', 'is', 'not'): 913,\n",
       " ('is', 'not', 'included'): 58,\n",
       " ('not', 'included', 'in'): 148,\n",
       " ('included', 'in', 'the'): 1475,\n",
       " ('in', 'the', 'constitution'): 58,\n",
       " ('the', 'constitution', 'we'): 10,\n",
       " ('constitution', 'we', 'are'): 2,\n",
       " ('we', 'are', 'having'): 214,\n",
       " ('are', 'having', 'these'): 2,\n",
       " ('having', 'these', 'speeches'): 1,\n",
       " ('these', 'speeches', 'made'): 1,\n",
       " ('speeches', 'made', 'in'): 8,\n",
       " ('made', 'in', 'an'): 8,\n",
       " ('in', 'an', 'extremely'): 72,\n",
       " ('an', 'extremely', 'bad'): 8,\n",
       " ('extremely', 'bad', 'mood'): 1,\n",
       " ('bad', 'mood', 'and'): 1,\n",
       " ('mood', 'and', 'striking'): 1,\n",
       " ('and', 'striking', 'a'): 3,\n",
       " ('striking', 'a', 'very'): 1,\n",
       " ('a', 'very', 'UNK'): 51,\n",
       " ('very', 'UNK', 'tone'): 1,\n",
       " ('UNK', 'tone', 'which'): 2,\n",
       " ('tone', 'which', 'are'): 1,\n",
       " ('which', 'are', 'not'): 712,\n",
       " ('are', 'not', 'appropriate'): 13,\n",
       " ('not', 'appropriate', 'in'): 12,\n",
       " ('appropriate', 'in', 'the'): 22,\n",
       " ('in', 'the', 'european'): 9986,\n",
       " ('the', 'european', 'parliament'): 22379,\n",
       " ('european', 'parliament', 'end1'): 2488,\n",
       " ('parliament', 'end1', 'start1'): 6143,\n",
       " ('end1', 'start1', 'turkey'): 340,\n",
       " ('start1', 'turkey', 'has'): 58,\n",
       " ('turkey', 'has', 'committed'): 3,\n",
       " ('has', 'committed', 'itself'): 71,\n",
       " ('committed', 'itself', 'to'): 109,\n",
       " ('itself', 'to', 'introducing'): 1,\n",
       " ('to', 'introducing', 'reforms'): 1,\n",
       " ('introducing', 'reforms', 'maintaining'): 1,\n",
       " ('reforms', 'maintaining', 'good'): 1,\n",
       " ('maintaining', 'good', 'relations'): 3,\n",
       " ('good', 'relations', 'with'): 80,\n",
       " ('relations', 'with', 'its'): 42,\n",
       " ('with', 'its', 'neighbours'): 67,\n",
       " ('its', 'neighbours', 'and'): 29,\n",
       " ('neighbours', 'and', 'progressively'): 1,\n",
       " ('and', 'progressively', 'aligning'): 1,\n",
       " ('progressively', 'aligning', 'itself'): 1,\n",
       " ('aligning', 'itself', 'with'): 8,\n",
       " ('itself', 'with', 'the'): 84,\n",
       " ('with', 'the', 'community'): 91,\n",
       " ('the', 'community', 'acquis'): 67,\n",
       " ('community', 'acquis', 'end1'): 17,\n",
       " ('acquis', 'end1', 'start1'): 158,\n",
       " ('start1', 'i', 'voted'): 3383,\n",
       " ('i', 'voted', 'against'): 1004,\n",
       " ('voted', 'against', 'the'): 1107,\n",
       " ('against', 'the', 'report'): 559,\n",
       " ('the', 'report', 'itself'): 102,\n",
       " ('report', 'itself', 'because'): 1,\n",
       " ('itself', 'because', 'i'): 2,\n",
       " ('because', 'i', 'feel'): 86,\n",
       " ('feel', 'that', 'more'): 6,\n",
       " ('that', 'more', 'centralisation'): 1,\n",
       " ('more', 'centralisation', 'means'): 1,\n",
       " ('centralisation', 'means', 'less'): 1,\n",
       " ('means', 'less', 'safety'): 2,\n",
       " ('less', 'safety', 'for'): 2,\n",
       " ('safety', 'for', 'smaller'): 1,\n",
       " ('for', 'smaller', 'member'): 3,\n",
       " ('smaller', 'member', 'states'): 52,\n",
       " ('member', 'states', 'less'): 3,\n",
       " ('states', 'less', 'democracy'): 1,\n",
       " ('less', 'democracy', 'less'): 1,\n",
       " ('democracy', 'less', 'involvement'): 1,\n",
       " ('less', 'involvement', 'of'): 1,\n",
       " ('involvement', 'of', 'people'): 7,\n",
       " ('of', 'people', 'at'): 23,\n",
       " ('people', 'at', 'a'): 9,\n",
       " ('at', 'a', 'local'): 46,\n",
       " ('a', 'local', 'and'): 6,\n",
       " ('local', 'and', 'national'): 47,\n",
       " ('and', 'national', 'level'): 125,\n",
       " ('national', 'level', 'end1'): 407,\n",
       " ('level', 'end1', 'start1'): 3848,\n",
       " ('start1', 'the', 'commissioner'): 368,\n",
       " ('the', 'commissioner', 'offers'): 1,\n",
       " ('commissioner', 'offers', 'his'): 1,\n",
       " ('offers', 'his', 'sympathy'): 1,\n",
       " ('his', 'sympathy', 'but'): 1,\n",
       " ('sympathy', 'but', 'we'): 1,\n",
       " ('but', 'we', 'do'): 402,\n",
       " ('do', 'not', 'need'): 680,\n",
       " ('not', 'need', 'sympathy'): 1,\n",
       " ('need', 'sympathy', 'end1'): 1,\n",
       " ('sympathy', 'end1', 'start1'): 26,\n",
       " ('end1', 'start1', 'this'): 48241,\n",
       " ('start1', 'this', 'would'): 1118,\n",
       " ('this', 'would', 'lead'): 48,\n",
       " ('would', 'lead', 'to'): 282,\n",
       " ('lead', 'to', 'the'): 531,\n",
       " ('to', 'the', 'criminalisation'): 8,\n",
       " ('the', 'criminalisation', 'of'): 36,\n",
       " ('criminalisation', 'of', 'the'): 6,\n",
       " ('of', 'the', 'users'): 19,\n",
       " ('the', 'users', 'who'): 4,\n",
       " ('users', 'who', 'would'): 3,\n",
       " ('who', 'would', 'then'): 3,\n",
       " ('would', 'then', 'need'): 6,\n",
       " ('then', 'need', 'to'): 22,\n",
       " ('need', 'to', 'make'): 680,\n",
       " ('to', 'make', 'their'): 162,\n",
       " ('make', 'their', 'purchases'): 3,\n",
       " ('their', 'purchases', 'from'): 1,\n",
       " ('purchases', 'from', 'criminals'): 1,\n",
       " ('from', 'criminals', 'who'): 1,\n",
       " ('criminals', 'who', 'trade'): 1,\n",
       " ('who', 'trade', 'in'): 2,\n",
       " ('trade', 'in', 'hard'): 2,\n",
       " ('in', 'hard', 'drugs'): 4,\n",
       " ('hard', 'drugs', 'end1'): 8,\n",
       " ('drugs', 'end1', 'start1'): 267,\n",
       " ('start1', 'i', 'share'): 307,\n",
       " ('i', 'share', 'the'): 368,\n",
       " ('share', 'the', 'concern'): 71,\n",
       " ('the', 'concern', 'underlying'): 1,\n",
       " ('concern', 'underlying', 'amendment'): 1,\n",
       " ('underlying', 'amendment', 'no'): 4,\n",
       " ('amendment', 'no', '5'): 192,\n",
       " ('no', '5', 'which'): 22,\n",
       " ('5', 'which', 'envisages'): 1,\n",
       " ('which', 'envisages', 'the'): 5,\n",
       " ('envisages', 'the', 'possibility'): 4,\n",
       " ('possibility', 'of', 'a'): 168,\n",
       " ('of', 'a', 'thirdcountry'): 4,\n",
       " ('a', 'thirdcountry', 'national'): 13,\n",
       " ('thirdcountry', 'national', 'travelling'): 1,\n",
       " ('national', 'travelling', 'immediately'): 1,\n",
       " ('travelling', 'immediately', 'within'): 1,\n",
       " ('immediately', 'within', 'the'): 1,\n",
       " ('within', 'the', 'UNK'): 51,\n",
       " ('the', 'UNK', 'area'): 50,\n",
       " ('UNK', 'area', 'end1'): 30,\n",
       " ('start1', 'the', 'council'): 2430,\n",
       " ('the', 'council', 'framework'): 21,\n",
       " ('council', 'framework', 'decision'): 60,\n",
       " ('framework', 'decision', 'on'): 175,\n",
       " ('decision', 'on', 'the'): 574,\n",
       " ('on', 'the', 'recognition'): 68,\n",
       " ('the', 'recognition', 'and'): 40,\n",
       " ('recognition', 'and', 'enforcement'): 41,\n",
       " ('and', 'enforcement', 'of'): 98,\n",
       " ('enforcement', 'of', 'judgments'): 29,\n",
       " ('of', 'judgments', 'in'): 28,\n",
       " ('judgments', 'in', 'criminal'): 16,\n",
       " ('in', 'criminal', 'matters'): 135,\n",
       " ('criminal', 'matters', 'involving'): 1,\n",
       " ('matters', 'involving', 'deprivation'): 1,\n",
       " ('involving', 'deprivation', 'of'): 4,\n",
       " ('deprivation', 'of', 'liberty'): 8,\n",
       " ('of', 'liberty', 'issued'): 1,\n",
       " ('liberty', 'issued', 'in'): 1,\n",
       " ('issued', 'in', 'other'): 3,\n",
       " ('in', 'other', 'eu'): 61,\n",
       " ('other', 'eu', 'member'): 103,\n",
       " ('eu', 'member', 'states'): 2019,\n",
       " ('member', 'states', 'is'): 572,\n",
       " ('states', 'is', 'the'): 42,\n",
       " ('is', 'the', 'subject'): 170,\n",
       " ('the', 'subject', 'of'): 2377,\n",
       " ('subject', 'of', 'mr'): 16,\n",
       " ('of', 'mr', 'varvitsiotis'): 2,\n",
       " ('mr', 'varvitsiotis', 'report'): 1,\n",
       " ('varvitsiotis', 'report', 'end1'): 1,\n",
       " ('report', 'end1', 'start1'): 8695,\n",
       " ('start1', 'we', 'will'): 2674,\n",
       " ('we', 'will', 'be'): 2318,\n",
       " ('will', 'be', 'voting'): 347,\n",
       " ('be', 'voting', 'on'): 143,\n",
       " ('voting', 'on', 'the'): 225,\n",
       " ('on', 'the', 'first'): 211,\n",
       " ('the', 'first', 'reading'): 428,\n",
       " ('first', 'reading', 'of'): 98,\n",
       " ('reading', 'of', 'this'): 34,\n",
       " ('of', 'this', 'regulation'): 299,\n",
       " ('this', 'regulation', 'in'): 28,\n",
       " ('regulation', 'in', 'june'): 2,\n",
       " ('in', 'june', 'end1'): 131,\n",
       " ('june', 'end1', 'start1'): 363,\n",
       " ('end1', 'start1', 'for'): 10541,\n",
       " ('start1', 'for', 'the'): 1533,\n",
       " ('for', 'the', 'sake'): 719,\n",
       " ('the', 'sake', 'of'): 714,\n",
       " ('sake', 'of', 'speed'): 1,\n",
       " ('of', 'speed', 'i'): 1,\n",
       " ('speed', 'i', 'shall'): 1,\n",
       " ('i', 'shall', 'set'): 4,\n",
       " ('shall', 'set', 'out'): 3,\n",
       " ('set', 'out', 'my'): 10,\n",
       " ...}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_prob_bi_gram(dict_bigram, dict_unigram, sentence)\n",
    "    sum_of_log_probs = 0\n",
    "    sent_length = len(test_sentence_tokens) -1\n",
    "    \n",
    "    for index, token in enumerate(test_sentence_tokens):\n",
    "        if index < sent_length:\n",
    "            sum_of_log_probs += np.log2(dict_bigrams[(token,test_sentence_tokens[index+1])]/dict_unigrams[token])\n",
    "\n",
    "    print(sum_of_log_probs)\n",
    "    sentence_prob = np.exp(sum_of_log_probs)\n",
    "    print(sentence_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89591"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Bigram algorithm for a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c350c71c3289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdev_tokens\u001b[0m \u001b[1;31m#'We Do not, like somEthing.!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSTART2_TOKEN\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mEND_TOKEN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_sentence_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_sentence_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sentence_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_sentence_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentence Prob with BiGram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('germans', 'may')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-39a8323e165d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msent_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msum_of_log_probs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_bigrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdict_unigrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_of_log_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('germans', 'may')"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
