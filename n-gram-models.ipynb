{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "Course's pdf\n",
    "<br>\n",
    "https://cs.nyu.edu/courses/spring17/CSCI-UA.0480-009/lecture3-and-half-n-grams.pdf\n",
    "<br>\n",
    "https://www.cs.bgu.ac.il/~elhadad/nlp18/hw1.html#crawl\n",
    "\n",
    "### Code\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/115676_ab6bb49748c742b88127e8b5ce3e1298.html\n",
    "<br>\n",
    "https://appliedmachinelearning.blog/2017/04/30/language-identification-from-texts-using-bi-gram-model-pythonnltk/\n",
    "<br>\n",
    "http://www.albertauyeung.com/post/generating-ngrams-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSc Data Science\n",
    "## Course: Natural Language Proccessing\n",
    "### 1st Assignement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done\n",
    "- Read the data(corpus)\n",
    "- Split the data into tokens(words)\n",
    "- Create unigram, bigram structures\n",
    "<br>\n",
    "### Undercunstrction\n",
    "- Decide how we will 'clean' the data (punctuations, capital letters, numbers etc)\n",
    "- Split the corpus to training and test data\n",
    "<br>\n",
    "### To-do\n",
    "- Trigram structure\n",
    "- Only words that occur, e.g., at least 10 times in the training subset\n",
    "- Deal with unseen and unknown words\n",
    "- Comput entropy and perplexity\n",
    "- Tune the model with the above metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import essential libraries\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#!pip install git+git://github.com/kootenpv/contractions.git\n",
    "import contractions\n",
    "#!pip install inflect\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Europarl corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the corpus\n",
    "input_path = r'C:\\Users\\User\\Desktop\\MSc Courses\\Untitled Folder\\europarl-v7.el-en.en'\n",
    "circlefile = open (input_path,encoding=\"utf8\")\n",
    "corpus = circlefile.read()\n",
    "circlefile.close()\n",
    "#Print a slice of the corpus\n",
    "corpus[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "source:\n",
    "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~5min\n",
    "corpus = denoise_text(corpus)\n",
    "corpus = replace_contractions(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokinize\n",
    "# import re\n",
    "# from nltk.util import ngrams\n",
    "# from nltk import bigrams, trigrams\n",
    "\n",
    "# corpus = corpus.lower()\n",
    "# corpus = re.sub(r'[^a-zA-Z0-9\\s]', ' ', corpus)\n",
    "# tokens = [token for token in corpus.split(\" \") if token != \"\"]\n",
    "# #output = list(ngrams(tokens, 2))\n",
    "tokens = nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    #words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    #words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~2 min\n",
    "tokens = normalize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resumption',\n",
       " 'of',\n",
       " 'the',\n",
       " 'session',\n",
       " 'i',\n",
       " 'declare',\n",
       " 'resumed',\n",
       " 'the',\n",
       " 'session',\n",
       " 'of',\n",
       " 'the',\n",
       " 'european',\n",
       " 'parliament',\n",
       " 'adjourned',\n",
       " 'on',\n",
       " 'friday',\n",
       " '17',\n",
       " 'december',\n",
       " '1999',\n",
       " 'and',\n",
       " 'i',\n",
       " 'would',\n",
       " 'like',\n",
       " 'once',\n",
       " 'again',\n",
       " 'to',\n",
       " 'wish',\n",
       " 'you',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'new',\n",
       " 'year',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'you',\n",
       " 'enjoyed',\n",
       " 'a',\n",
       " 'pleasant',\n",
       " 'festive',\n",
       " 'period',\n",
       " 'although',\n",
       " 'as',\n",
       " 'you',\n",
       " 'will',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'the',\n",
       " 'dreaded',\n",
       " 'millennium',\n",
       " 'bug',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'materialise',\n",
       " 'still',\n",
       " 'the',\n",
       " 'people',\n",
       " 'in',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'countries',\n",
       " 'suffered',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'disasters',\n",
       " 'that',\n",
       " 'truly',\n",
       " 'were',\n",
       " 'dreadful',\n",
       " 'you',\n",
       " 'have',\n",
       " 'requested',\n",
       " 'a',\n",
       " 'debate',\n",
       " 'on',\n",
       " 'this',\n",
       " 'subject',\n",
       " 'in',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " 'the',\n",
       " 'next',\n",
       " 'few',\n",
       " 'days',\n",
       " 'during',\n",
       " 'this',\n",
       " 'partsession',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " 'i',\n",
       " 'should',\n",
       " 'like',\n",
       " 'to',\n",
       " 'observe']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram/Bigram/Trigram Structures\n",
    "\n",
    "usefull sources:\n",
    "https://github.com/ollie283/language-models/blob/master/LangModel.py\n",
    "<br>\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/115676_ab6bb49748c742b88127e8b5ce3e1298.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Resumption',\n",
       " 'of',\n",
       " 'the',\n",
       " 'session',\n",
       " 'I',\n",
       " 'declare',\n",
       " 'resumed',\n",
       " 'the',\n",
       " 'session',\n",
       " 'of']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # used for unseen words in training vocabularies\n",
    "# UNK = None\n",
    "# # sentence start and end\n",
    "# SENTENCE_START = \"<s>\"\n",
    "# SENTENCE_END = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a list of tokens, return a dictionary where key are the existing unique tokens\n",
    "# and values are their counts; also return the size N of a genre's corpus\n",
    "def get_uniCounts(all_tokens):\n",
    "    unigram_table = {}\n",
    "    for token in all_tokens:\n",
    "        if token in unigram_table:\n",
    "            unigram_table[token] += 1\n",
    "        else:\n",
    "            unigram_table[token] = 1\n",
    "    return unigram_table, len(all_tokens)\n",
    "\n",
    "# given a genre, return a \"dictionary with dictionaries inside\" and the # of seen bigrams\n",
    "# where outside keys are the first words in existing bigrams,\n",
    "# and values are dictionaries with the subsequent word as key\n",
    "# and counts of such bigram as value\n",
    "def get_biCounts(all_tokens):\n",
    "    uniCounts, length = get_uniCounts(all_tokens)\n",
    "    bigram_table = {}\n",
    "    num_bigrams = 0\n",
    "    for x in range(0, length - 1):\n",
    "        if all_tokens[x] in bigram_table:\n",
    "            if all_tokens[x + 1] in bigram_table[all_tokens[x]]:\n",
    "                bigram_table[all_tokens[x]][all_tokens[x + 1]] += 1\n",
    "            else:\n",
    "                bigram_table[all_tokens[x]][all_tokens[x + 1]] = 1\n",
    "                num_bigrams += 1\n",
    "        else:\n",
    "            bigram_table[all_tokens[x]] = {}\n",
    "            bigram_table[all_tokens[x]][all_tokens[x + 1]] = 1\n",
    "            num_bigrams += 1\n",
    "    return bigram_table, num_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~2min\n",
    "table_uniCounts = get_uniCounts(tokens)\n",
    "table_biCounts = get_biCounts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_uniCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_biCounts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
